{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sae_auto_interp.explainers.cot.cot import ChainOfThought, ExplainerInput\n",
    "from sae_auto_interp.clients import get_client\n",
    "\n",
    "client = get_client(\"local\", \"astronomer/Llama-3-8B-Instruct-GPTQ-8-Bit\")\n",
    "explainer = ChainOfThought(client)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"astronomer/Llama-3-8B-Instruct-GPTQ-8-Bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : \"List the names of cities around the world. please return your response in json.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "value = await client.async_generate(\n",
    "    prompt,\n",
    "    # extra_body=dict(guided_json=json_template)\n",
    "    response_format={'type': 'json_object'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : \"Think two math word problems and their response.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "tokenized_prompt =tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Dict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ExampleResponseDynamic(BaseModel):\n",
    "    question_1: int\n",
    "    question_2: int\n",
    "\n",
    "url = \"http://127.0.0.1:8000/generate\"\n",
    "\n",
    "data = {\n",
    "    \"prompt\": tokenized_prompt,\n",
    "    \"schema\": ExampleResponseDynamic.model_json_schema(),\n",
    "    \"max_tokens\" : 50\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# if response.status_code == 200:\n",
    "#     print(\"Response:\", response.json())\n",
    "# else:\n",
    "#     print(\"Failed to get response. Status code:\", response.status_code)\n",
    "#     print(\"Response text:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nThink two math word problems and their response.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{ \"question_1\": 5, \"question_2\": 2}']}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Dict\n",
    "\n",
    "class ExampleResponseDynamic(BaseModel):\n",
    "    responses: Dict[str, int]\n",
    "\n",
    "# Create an instance of the model with the given data\n",
    "response_data_dynamic = {\n",
    "    \"responses\": {\n",
    "        \"example_1\": 1,\n",
    "        \"example_2\": 0,\n",
    "        \"example_3\": 1,\n",
    "        \"example_4\": 1,\n",
    "        \"example_5\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "example_response_dynamic = ExampleResponseDynamic(**response_data_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, create_model\n",
    "\n",
    "def create_dynamic_model(num_questions):\n",
    "    fields = {f\"question_{i}\": (int, ...) for i in range(1, num_questions + 1)}\n",
    "    return create_model(\"DynamicQuestionModel\", **fields)\n",
    "\n",
    "# Create a dynamic model with 5 questions\n",
    "DynamicQuestionModel = create_dynamic_model(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'question_1': {'title': 'Question 1', 'type': 'integer'},\n",
       "  'question_2': {'title': 'Question 2', 'type': 'integer'},\n",
       "  'question_3': {'title': 'Question 3', 'type': 'integer'},\n",
       "  'question_4': {'title': 'Question 4', 'type': 'integer'},\n",
       "  'question_5': {'title': 'Question 5', 'type': 'integer'}},\n",
       " 'required': ['question_1',\n",
       "  'question_2',\n",
       "  'question_3',\n",
       "  'question_4',\n",
       "  'question_5'],\n",
       " 'title': 'DynamicQuestionModel',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DynamicQuestionModel.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/caden/.conda/envs/autointerp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/share/u/caden/.conda/envs/autointerp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sae_auto_interp.autoencoders.ae import load_autoencoders\n",
    "from sae_auto_interp.features.cache import FeatureCache\n",
    "\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"auto\", dispatch=True)\n",
    "\n",
    "ae_dict, submodule_dict, edits = load_autoencoders(\n",
    "    model, \n",
    "    \"/share/u/caden/sae-auto-interp/sae_auto_interp/autoencoders/oai/gpt2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import utils\n",
    "from datasets import load_dataset\n",
    "from nnsight import LanguageModel \n",
    "\n",
    "data = load_dataset(\"stas/openwebtext-10k\", split=\"train\")\n",
    "\n",
    "tokens = utils.tokenize_and_concatenate(\n",
    "    data, \n",
    "    model.tokenizer, \n",
    "    max_length=64\n",
    ")   \n",
    "\n",
    "tokens = tokens.shuffle(22)['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<sae_auto_interp.features.features.FeatureRecord object at 0x7f143922e650>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sae_auto_interp.features.features import feature_loader, Feature\n",
    "\n",
    "features_path = \"/share/u/caden/sae-auto-interp/saved_records\"\n",
    "layer_index = 6\n",
    "feature_index = 13452\n",
    "\n",
    "feature = Feature(\n",
    "    layer_index,\n",
    "    feature_index\n",
    ")\n",
    "\n",
    "\n",
    "for ae, records in feature_loader(\n",
    "    tokens, \n",
    "    [feature],\n",
    "    model,\n",
    "    ae_dict,\n",
    "    features_path\n",
    "):\n",
    "    print(records)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' technical discipling of one era of the working class is the result of their struggle in the preceding era'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0].examples[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = records[0]\n",
    "record.top_logits = [\"penis\", \"PENIS\", \"penis\", \"PENISPENIS\"]\n",
    "\n",
    "explainer_in = ExplainerInput(\n",
    "    record.examples,\n",
    "    record,\n",
    ")\n",
    "\n",
    "value = await explainer(explainer_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The token \"er\" at the end of a comparative adjective describing size.\\n\\n**Part 1**\\n\\nStep 1:\\nThe activating tokens are mostly \"er\".\\nThe previous tokens are mostly adjectives, or parts of adjectives, describing size.\\nThe next tokens have nothing in common.\\nThe neuron seems to activate on, or near, the token \"er\" in comparative adjectives describing size.\\n\\nStep 2:\\n- In each example, the activating token was \"er\" appearing at the end of a comparative adjective.\\n- The comparative adjectives (\"wider\", \"taller\", \"smaller\", \"deeper\") all describe size.\\n\\nLet me look again for patterns in the examples. Are there any links or hidden linguistic commonalities that I missed? I can\\'t see any.\\n\\n**Part 2**\\n\\nStep 3:\\nSIMILAR TOKENS: None.\\nThe top logits list contains mostly unrelated nouns and adverbs.\\n\\nStep 4:\\n[EXPLANATION]: The token \"er\" at the end of a comparative adjective describing size.\\n\\n**Part 1**\\n\\nStep 1:\\nThe activating tokens are all things that one can be in.\\nThe previous tokens have nothing in common.\\nThe next tokens are all quotation marks.\\n\\nStep 2:\\n- The examples involve being inside something, sometimes figuratively.\\n- The activating token is a thing which something else is inside of.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities? Yes, I missed one:\\n\\n- The activating token is followed by a quotation mark, suggesting it occurs within speech.\\n\\n**Part 2**\\n\\nStep 3:\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\nThe top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n\\nStep 4:\\n[EXPLANATION]: Nouns preceding a quotation mark, representing a thing that contains something.\\n\\nNow, let\\'s analyze the neuron\\'s behavior:\\n\\n**Part 1**\\n\\nThe activating tokens are all parts of common idioms.\\nThe previous tokens have nothing in common.\\nThe next tokens are sometimes exclamation marks.\\n\\n**Part 2**\\n\\nThe top logits list contains words that are strongly associated with positive emotions.\\n\\n**Part 1**\\n\\nThe activating tokens are mostly \"er\".\\nThe previous tokens are mostly adjectives, or parts of adjectives, describing size.\\nThe next tokens have nothing in common.\\nThe neuron seems to activate on, or near, the token \"er\" in comparative adjectives describing size.\\n\\n**Part 2**\\n\\nThe top logits list contains mostly unrelated nouns and adverbs.\\n\\n**Part 1**\\n\\nThe activating tokens are all things that one can be in.\\nThe previous tokens have nothing in common.\\nThe next tokens are all quotation marks.\\n\\n**Part 2**\\n\\nThe top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n\\n**Explanation**\\n\\n[EXPLANATION]: The neuron is activated by common idioms, comparative adjectives describing size, and nouns representing physical or metaphorical spaces, which are often followed by quotation marks, suggesting they occur within speech.\\n\\nPlease note that the explanations are based on the provided examples and may not be generalizable to all cases.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.explanation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autointerp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
